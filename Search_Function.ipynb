{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOac5wa/2+3KUvx/Z+hDDFM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamMeacham/cardcutting/blob/main/Search_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynxVvZJfUSTV",
        "outputId": "ddc46731-6fd4-480d-985d-dcce6c70b38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfnvwcZspJMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "INhsglGhpJj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Card Cutting"
      ],
      "metadata": {
        "id": "mYlFsMQ1pIzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from openai import OpenAI\n",
        "os.environ['OPENAI_API_KEY'] = ###KEY##\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an assistant specialized in summarizing and highlighting key information from extensive texts. Focus on extracting the most important lines based on the topics specified by the user.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Identify the most important lines from the following article, emphasizing the topics [insert topics here]. Here's the article: [insert article content here].\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "tU_qczj6aRVY",
        "outputId": "2c5083b8-f37b-4436-9e82-a9841c52ff75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-81ijP***************************************Lpab. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-c3ee25b6ac43>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'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'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    668\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         )\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 897\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-81ijP***************************************Lpab. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"But mostly diseases don't drive species extinct. There are several reasons for that. For one, the most dangerous diseases are those that spread from one individual to another. If the disease is highly lethal, then the population drops, and it becomes less likely that individuals will contact each other during the infectious phase. Highly contagious diseases tend to burn themselves out that way.¶ Probably the main reason is variation. Within the host and the pathogen population there will be a wide range of variants. Some hosts may be naturally resistant. Some pathogens will be less virulent. And either alone or in combination, you end up with infected individuals who survive.¶ We see this in HIV, for example. There is a small fraction of humans who are naturally resistant or altogether immune to HIV, either because of their CCR5 allele or their MHC Class I type. And there are a handful of people who were infected with defective versions of HIV that didn't progress to disease. ¶ We can see indications of this sort of thing happening in the past, because our genomes contain many instances of pathogen resistance genes that have spread through the whole population. Those all started off as rare mutations that conferred a strong selection advantage to the carriers, meaning that the specific infectious diseases were serious threats to the species.\""
      ],
      "metadata": {
        "id": "8NBOGuhDfS8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Underlining\n",
        "def underline(client, article_text, topics):\n",
        "    \"\"\"\n",
        "    Calls the GPT API to extract the most important lines from an article,\n",
        "    focusing on specified topics.\n",
        "\n",
        "    Parameters:\n",
        "    - client: The instance of the OpenAI API client.\n",
        "    - article_text: The full text of the article to analyze.\n",
        "    - topics: A list of topics or arguments the user is interested in.\n",
        "\n",
        "    Returns:\n",
        "    A string containing the summarized important lines as extracted by the API.\n",
        "    \"\"\"\n",
        "    # Ensure topics is a list for proper processing\n",
        "    if isinstance(topics, str):\n",
        "        topics = [topics]\n",
        "\n",
        "    # Convert the list of topics into a comma-separated string\n",
        "    topics_str = \", \".join(topics)\n",
        "\n",
        "    # Construct the API call\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in extracting key lines from news articles and policy briefs. You are focused on brevity and should only give me back the most important sentences based on the arguments specified by the user. Do not modify or edit the text in any way. You should only choose the most important lines as they are represented in the text\"},\n",
        "            {\"role\": \"user\", \"content\": f\"I want lines that prove and support the following arguments: {topics_str}. Here's the article: {article_text}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Correct way to access the content of the first choice's message\n",
        "    highlighted_lines = response.choices[0].message.content if response.choices else \"\"\n",
        "\n",
        "    return highlighted_lines\n"
      ],
      "metadata": {
        "id": "-UEgkyojfSIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "BnR3TeXBpV3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#highlight\n",
        "def highlight(client, underlined, topics):\n",
        "    \"\"\"\n",
        "    Takes important lines extracted from an article and distills them into\n",
        "    the most critical phrases or words to construct an argument.\n",
        "\n",
        "    Parameters:\n",
        "    - client: The instance of the OpenAI API client.\n",
        "    - important_lines: A string containing the important lines as extracted by the previous API call.\n",
        "    - topics: A list of topics or arguments to guide the extraction of key phrases.\n",
        "\n",
        "    Returns:\n",
        "    A string with the most critical phrases or words highlighted.\n",
        "    \"\"\"\n",
        "    # Ensure topics is a list for proper processing\n",
        "    if isinstance(topics, str):\n",
        "        topics = [topics]\n",
        "\n",
        "    # Convert the list of topics into a comma-separated string\n",
        "    topics_str = \", \".join(topics)\n",
        "\n",
        "    # Construct the API call with a new prompt\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in identifying and highlighting key phrases within specific sentences. Your goal is to extract and highlight the phrases that are most crucial for constructing an argument based on specified topics. Highlight only the essential phrases.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"From the following important lines, extract and highlight the phrases most crucial for supporting the arguments: {topics_str}. Here are the important lines: {underlined}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extracting the highlighted phrases from the response\n",
        "    highlighted_phrases = response.choices[0].message.content if response.choices else \"\"\n",
        "\n",
        "    return highlighted_phrases\n",
        "\n"
      ],
      "metadata": {
        "id": "SFokhAOUjd5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "underlined = underline(client, article, \"I want lines that prove that diseases will not cause extinction because they kill their hosts too quickly\")"
      ],
      "metadata": {
        "id": "5m8IGuXjlTYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highlighted = highlight(client, underlined,  \"I want lines that prove that diseases will not cause extinction because they kill their hosts too quickly\")"
      ],
      "metadata": {
        "id": "1iFskV7GlYAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(underlined)\n",
        "print(highlighted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "N-4sloHlmPI2",
        "outputId": "cdc6cd0a-0b4d-448d-beec-9bff6efbb3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'docx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-e4efd355652e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRGBColor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWD_COLOR_INDEX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexport_highlighted_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munderlined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import RGBColor, Pt\n",
        "from docx.enum.text import WD_COLOR_INDEX, WD_UNDERLINE\n",
        "\n",
        "def format_article_with_highlights(document_text, important_lines, highlighted_phrases):\n",
        "    \"\"\"\n",
        "    Creates a Word document from the original article text, with specific\n",
        "    lines underlined and specific phrases highlighted.\n",
        "\n",
        "    Parameters:\n",
        "    - document_text: The full text of the original article.\n",
        "    - important_lines: A list of important lines to be underlined.\n",
        "    - highlighted_phrases: A list of phrases to be highlighted.\n",
        "\n",
        "    Saves the document as 'formatted_article.docx'.\n",
        "    \"\"\"\n",
        "    doc = Document()\n",
        "    para = doc.add_paragraph()\n",
        "\n",
        "    # Split the document into lines for easier processing\n",
        "    lines = document_text.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        added = False\n",
        "        # Check if the line is one of the important lines\n",
        "        if line in important_lines:\n",
        "            run = para.add_run(line + '\\n')\n",
        "            run.underline = True\n",
        "            added = True\n",
        "\n",
        "        # Check for and highlight phrases within the line\n",
        "        for phrase in highlighted_phrases:\n",
        "            if phrase in line:\n",
        "                # If the line hasn't been added already\n",
        "                if not added:\n",
        "                    start_index = line.find(phrase)\n",
        "                    before_phrase = line[:start_index]\n",
        "                    # Add text before the phrase without formatting\n",
        "                    if before_phrase:\n",
        "                        para.add_run(before_phrase)\n",
        "                    # Add the phrase with highlighting\n",
        "                    highlighted_run = para.add_run(phrase)\n",
        "                    highlighted_run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
        "                    # Add remaining part of the line\n",
        "                    after_phrase = line[start_index + len(phrase):]\n",
        "                    if after_phrase:\n",
        "                        para.add_run(after_phrase + '\\n')\n",
        "                else:\n",
        "                    # If the line was already added because it's important, highlight the phrase\n",
        "                    highlighted_run = para.add_run(phrase)\n",
        "                    highlighted_run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
        "                added = True\n",
        "                break  # Assuming one phrase per line for simplicity\n",
        "\n",
        "        # If the line was neither important nor contained a highlighted phrase, add it as is\n",
        "        if not added:\n",
        "            para.add_run(line + '\\n')\n",
        "\n",
        "    doc.save('formatted_article.docx')\n",
        "    print(\"Document 'formatted_article.docx' has been created and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "5mCWI1oDo_pj",
        "outputId": "56e1b6a7-ac14-41b3-8b73-ada514f913d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'docx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-f5349e570a9d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRGBColor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWD_COLOR_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWD_UNDERLINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mformat_article_with_highlights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportant_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlighted_phrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search"
      ],
      "metadata": {
        "id": "i38gCzk1pO1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install exa_py\n",
        "from exa_py import Exa\n",
        "exa = Exa(\"447f6591-0c8c-4a5e-8c52-8d35a5cd5d1f\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AbdhvkMr8Wj",
        "outputId": "19fb5328-1ba4-4a52-f929-e91912d74a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: exa_py in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from exa_py) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from exa_py) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->exa_py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->exa_py) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->exa_py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->exa_py) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "os.environ['OPENAI_API_KEY'] = #####\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "7ShfZXmRGCq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translatetometaphor(task):\n",
        "    \"\"\"\n",
        "    Calls GPT to transform a detailed task description into a one-sentence summary.\n",
        "\n",
        "    Parameters:\n",
        "    - task: A string describing the user's specific interest or query.\n",
        "\n",
        "    Returns:\n",
        "    A string containing a one-sentence description of the desired article.\n",
        "    \"\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in shortening long descriptions of desired articles into a one-sentence description of the desired article. Focus on retaining the key components of the desired article. Please keep the sentence relatively short, around at least less than 15 or 20 words\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Please transform the following search term into a one-sentence description of the desired article: {task}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    shortened_task = completion.choices[0].message.content\n",
        "    return shortened_task\n"
      ],
      "metadata": {
        "id": "-gFQZyuUpW0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"I want an article that argues that diseases will not cause extinction. The article should argue that disease will not kill everyone.\"\n",
        "print(translatetometaphor(task))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWOERRXF_8eG",
        "outputId": "a31512ea-06a3-45aa-fb35-3f8f0c6be835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article arguing diseases won't cause human extinction but won't kill everyone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_relevant_article(texts, task):\n",
        "    \"\"\"\n",
        "    Calls GPT to compare the texts of ten articles against a user-defined task and identify the most relevant article.\n",
        "\n",
        "    Parameters:\n",
        "    - client: The instance of the OpenAI API client.\n",
        "    - texts: A list of strings, where each string is the text of an article.\n",
        "    - task: A string describing the user's specific interest or query.\n",
        "\n",
        "    Returns:\n",
        "    An integer index of the article in the `texts` list that is most relevant to the task.\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine all texts into a single string, each preceded by its index for identification\n",
        "    combined_texts = \"\\n\\n\".join([f\"Article {i+1}: {text}\" for i, text in enumerate(texts[:5])])\n",
        "\n",
        "    # Construct the API call with a custom prompt\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in analyzing texts to determine which one most closely matches a specified topic or task. Identify the most relevant text based on the task description.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Task: {task}\\n\\n{combined_texts}\\n\\nWhich article is most relevant to the above task? In your output, provide solely the number of the article you chose. Do not provide anything else besides the number. Do not say 'Article' before the number. Only put the number, so there should be no letters in your response.\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Assuming the response is in the expected format, parse it to find the most relevant article\n",
        "    if response.choices and response.choices[0].message.content:\n",
        "      return response.choices[0].message.content\n",
        "    else:\n",
        "        return \"Unable to determine the most relevant article due to an unexpected response format.\"\n",
        "\n",
        "# Usage example:\n",
        "# Assuming `client` is your OpenAI client instance and you have the `texts` list and `task` string defined\n",
        "# relevant_article = find_most_relevant_article(client, texts, task)\n",
        "# print(relevant_article)\n"
      ],
      "metadata": {
        "id": "Bi3rKqcd0uiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"I want an article that argues the tik tok ban will not be effective\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "HeezBql607bd",
        "outputId": "8760ae84-922c-41a2-88f6-9e69436a8956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "find_most_relevant_article() takes 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-215-129c72c0d929>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I want an article that argues the tik tok ban will not be effective\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgpt_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_most_relevant_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: find_most_relevant_article() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article_info(gpt_index, titles, urls, authors, texts, dates):\n",
        "    \"\"\"\n",
        "    Constructs a dictionary with the information of the chosen article based on the provided GPT output index,\n",
        "    automatically adjusting for zero-based indexing.\n",
        "\n",
        "    Parameters:\n",
        "    - gpt_index: The one-based index of the chosen article as provided by GPT.\n",
        "    - titles: A list of article titles.\n",
        "    - urls: A list of article URLs.\n",
        "    - authors: A list of article authors.\n",
        "    - texts: A list of article texts.\n",
        "    - dates: A list of article publication dates.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary containing the title, URL, author, text, and publication date of the chosen article, after adjusting for zero-based indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Adjust for zero-based indexing\n",
        "    index = int(gpt_index) - 1\n",
        "\n",
        "    # Ensure the index is within the bounds of the lists\n",
        "    if 0 <= index < len(titles):\n",
        "        article_info = {\n",
        "            \"title\": titles[index],\n",
        "            \"url\": urls[index],\n",
        "            \"author\": authors[index],\n",
        "            \"text\": texts[index],\n",
        "            \"date\": dates[index]\n",
        "        }\n",
        "        return article_info\n",
        "    else:\n",
        "        return {\"error\": \"Index out of range. Please provide a valid article number.\"}\n",
        "\n",
        "# Example usage:\n",
        "# Assuming the lists `titles`, `urls`, `authors`, `texts`, and `dates` are defined, and `gpt_index` is the index from GPT output\n",
        "# gpt_index = int(get_gpt_response(client, texts, task))  # Ensure you convert GPT's response to an integer\n",
        "# article_info = get_article_info(gpt_index, titles, urls, authors, texts, dates)\n",
        "# print(article_info)\n"
      ],
      "metadata": {
        "id": "CKylYLEQ68W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"I want an article that argues disease won't cause extinction. The article provides arguments that disease will not kill all humans and gives specific reasons why disease will not kill everyone\"\n",
        "def finalsearch(task):\n",
        "  shortenedtask = translatetometaphor(task)\n",
        "\n",
        "  search_response = exa.search_and_contents(shortenedtask, use_autoprompt=True)\n",
        "\n",
        "  titles = [result.title for result in search_response.results]\n",
        "  urls = [result.url for result in search_response.results]\n",
        "  authors = [result.author for result in search_response.results]\n",
        "  texts = [result.text for result in search_response.results]\n",
        "  date = [result.published_date for result in search_response.results]\n",
        "\n",
        "  gpt_index = find_most_relevant_article(texts, task)\n",
        "  print(gpt_index)\n",
        "\n",
        "  dictionary = get_article_info(gpt_index, titles, urls, authors, texts, date)\n",
        "\n",
        "  return(dictionary)\n",
        "\n"
      ],
      "metadata": {
        "id": "u5Gpsit07ePq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Here is an article arguing that the tik tok ban is ineffective\"\n",
        "test = finalsearch(task)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_KPIZsUDa71",
        "outputId": "48bc2f1e-c079-426a-beee-2d0c4c8bb887"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CrYYttOGsN-",
        "outputId": "1a32f9f7-13cf-40a5-ca23-55f838b14982"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Opinion | Banning TikTok is a bad solution to the wrong problem', 'url': 'https://www.washingtonpost.com/opinions/2023/03/15/tiktok-ban-propaganda-bytedance-china/', 'author': 'Chris Stokel-Walker', 'text': '    Chris Stokel-Walker is a British journalist who writes about technology. He is the author of “TikTok Boom: China’s Dynamite App and the Superpower Race for Social Media.”  No more glow-ups. No more 10-second viral dances. No more short lessons on how to clean your stove. Banning TikTok would potentially save users a lot of time scrolling through incredibly addictive content.  But a ban would be an entirely un-American, undemocratic and inappropriate response to an unproven risk that the Chinese-owned platform will share users’ data with Beijing for nefarious purposes. What’s more, banning TikTok would be completely useless in combating a different, much better-evidenced social media pitfall — the spread of dangerous propaganda. An outright ban of the social media app in the United States is moving closer. Last week, the Biden administration threw its support behind a Senate bill that would give the Commerce Department that power. Cross-party consensus is building around the Restricting the Emergence of Security Threats That Risk Information and Communications Technology, or Restrict, Act. The bill follows moves by more than two dozen states that have banned their employees from using TikTok on official work devices. The federal government has done the same, and there have been similar measures in Europe and Canada. There’s an argument that government workers shouldn’t be mindlessly scrolling through social media on their work phones. For some, time spent on TikTok is time spent not working, though, of course, many government organizations use the app as a way to connect with users. But what about the rest of us? The worry is that TikTok’s owner, ByteDance, would hand over data to the Chinese government. Then ranks of analysts in Beijing would use that information, gleaned through exhaustive viewing of our video consumption habits, against us — though it’s not clear how. It’s also not clear why this is a worry at all. TikTok’s U.S. executives have repeatedly denied that they have been pressed by the Chinese government to share data and say they would refuse to do so if the request ever came. Reporters, including me, have tried for years to prove these executives are not telling the truth but have turned up nothing. Current and former TikTok staff members have dished plenty of dirt on the company to me, but no one has ever confirmed a great data transfer. Of course, even if ByteDance hasn’t handed over data, who’s to say it wouldn’t in the future? But is that any different for any number of social media companies, many of whom have extensive, valuable operations in China? Meta, the company behind Facebook and Instagram, was recently fined for mishandling user data under European Union rules. Meanwhile, TikTok is bending over backward to assuage fears. Project Texas involves a U.S. subsidiary that would control U.S. user data and answer directly to the Committee on Foreign Investment in the United States, while its European equivalent, Project Clover, would do something similar in Europe. Neither of these projects is likely to satisfy the knee-jerk, anti-China crowd. And maybe none of this would matter if TikTok were less popular or successful. But the platform now has more than 1 billion active monthly users, including the next generation of scientists, educators and politicians. Aging politicians might not get it. But TikTok has become a valued place for learning and communication, as well as online innovation. Banning it would be exactly the kind of suppression of freedom and censoring of the internet that we have come to expect of China, not the United States. The other concern is that bad actors, including the Chinese government, could use TikTok to foment their own ideas, lacing funny little videos with pages from Mao Zedong’s “Little Red Book.” Certainly, the evidence for social media as propaganda tool is much stronger. It already happens. On Facebook. On Instagram. On Twitter. And yes: on TikTok. This issue is real. But it’s hardly unique to TikTok. Indeed, TikTok is doing as much as any platform to deal with this particular problem. There is certainly more work to be done in the area of influence and misinformation. Despite government inquiries and investigative journalism projects, we still don’t fully understand the impact of social media on democratic elections and in promoting division and unrest. This has been one of the biggest societal stories of the past half-dozen years or so. Governments are only just coming to grips with it. But that is not a problem specific to one platform. A very compelling argument exists for more careful consideration of the role of social media, including TikTok, in our societies more broadly. Banning one platform and ignoring the others would solve nothing, while allowing the deeper problems to fester.      Opinion|Utah is limiting kids’ social media access. Other states should follow.       Opinion|Utah is limiting kids’ social media access. Other states should follow.       Opinion|Why banning TikTok won’t do any good       Opinion|Why banning TikTok won’t do any good       Opinion|Elon Musk excommunicates the priestly caste of Twitter       Opinion|Elon Musk excommunicates the priestly caste of Twitter       ', 'date': '2023-03-15'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpt_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWQO0pbBFhKR",
        "outputId": "d16b6137-491d-4b72-dbd6-20ddd315fabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = get_article_info(gpt_index, titles, urls, authors, texts, dates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "sPgq1arVFlSP",
        "outputId": "61bf5420-5335-4ee7-8fab-41bb898bb265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dates' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-205-030c22ba84c2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_article_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dates' is not defined"
          ]
        }
      ]
    }
  ]
}