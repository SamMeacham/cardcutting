{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X8cTfcwxRacC"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import warnings\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9RL0Nvd4lVbm",
        "outputId": "f53a94fe-00b3-4d33-bb9f-e26816fe7d08"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              ".output_area {\n",
              "    overflow-wrap: break-word;\n",
              "    word-wrap: break-word;\n",
              "    white-space: pre-wrap;\n",
              "    word-break: break-all;\n",
              "    white-space: normal;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        ".output_area {\n",
        "    overflow-wrap: break-word;\n",
        "    word-wrap: break-word;\n",
        "    white-space: pre-wrap;\n",
        "    word-break: break-all;\n",
        "    white-space: normal;\n",
        "}\n",
        "</style>\n",
        "\"\"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9UOShREisaeM"
      },
      "outputs": [],
      "source": [
        "def check_sub(small, large):\n",
        "    # Split the smaller string into sentences based on punctuation\n",
        "    small_split = re.split(r'(?<=[.!?]) +', small)\n",
        "\n",
        "    # Initialize a flag to track if all substrings are found\n",
        "    all_found = True\n",
        "\n",
        "    # Check each sentence in small_split against the large string\n",
        "    for ele in small_split:\n",
        "        if ele not in large:\n",
        "            warnings.warn(f\"Sentence not in article: sentence {small_split.index(ele)}, '{ele}'\")\n",
        "            all_found = False\n",
        "\n",
        "    return all_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ccz87bJDfU24"
      },
      "outputs": [],
      "source": [
        "assignment = \"You are an AI model designed to assist with \\'card-cutting\\' for policy debate. You may NOT modify the text by deleting or adding any words to the chosen sentences. You may only leave out the sentences you deem unimportant. Within each sentence you deem important enough to return, you MAY NOT remove ANY words from the sentence, nor change punctuation or capitalization. In fact, your return, if broken into individual sentences, should ensure that each and every returned sentence matches EXACTLY with a given sentence in the initial article. You will fail at your task if you do not follow these instructions. Your goal is to take the text of an article, understand what the article's key arguments are, and return a response including ONLY the key sentences based on your reading. Pick out the key parts according to the following request:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RFOXxvChw47_"
      },
      "outputs": [],
      "source": [
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u-9OwhaEzBCw"
      },
      "outputs": [],
      "source": [
        "def get_key_sentences(task, article):\n",
        "  completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": assignment + task},\n",
        "    {\"role\": \"user\", \"content\": article}\n",
        "  ]\n",
        ")\n",
        "\n",
        "  key_sentences = completion.choices[0].message.content\n",
        "  return key_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9HLOyMdjwl1T"
      },
      "outputs": [],
      "source": [
        "#highlight\n",
        "def highlight(key_sentences, task):\n",
        "    \"\"\"\n",
        "    Takes important lines extracted from an article and distills them into\n",
        "    the most critical phrases or words to construct an argument.\n",
        "\n",
        "    Parameters:\n",
        "    - important_lines: A string containing the important lines as extracted by the previous API call.\n",
        "    - topics: A list of topics or arguments to guide the extraction of key phrases.\n",
        "\n",
        "    Returns:\n",
        "    A string with the most critical phrases or words highlighted.\n",
        "    \"\"\"\n",
        "    # Construct the API call with a new prompt\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in identifying and highlighting key phrases within specific sentences. Your goal is to extract and highlight the phrases that are most crucial for constructing a well-warranted argument based on specified topics. Highlight the essential phrases but ensure you do not exclude reasoning, argumentative warrants, and key examples in the key sentences. As a rule of thumb, try to highlight a third OR MORE of the article depending on how dense it is in arguments. The highlighting ought to be readable and meaningful in terms of conveying information but not overly wordy. Return the selected phrases IN A PYTHON LIST with each key phrase being an element of that list. You will fail at your task if there is anything outside the individual strings in the list. PLEASE make sure each string that is a list element is enclosed in DOUBLE quotation marks like \\\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"From the following important lines, extract and highlight the phrases most crucial for the argument based on the following: {task}. Here are the important lines: {key_sentences}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extracting the highlighted phrases from the response\n",
        "    highlighted_phrases = response.choices[0].message.content if response.choices else \"\"\n",
        "\n",
        "\n",
        "    highlighted_phrases = ast.literal_eval(highlighted_phrases)\n",
        "\n",
        "    return highlighted_phrases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from docx import Document\n",
        "from docx.shared import RGBColor\n",
        "from docx.enum.text import WD_COLOR_INDEX\n",
        "\n",
        "def final_cut(article, keywords, doc_path, color=WD_COLOR_INDEX.YELLOW):\n",
        "    # Load the existing Word document\n",
        "    doc = Document(doc_path)\n",
        "    \n",
        "    # Add a new paragraph at the end of the document\n",
        "    paragraph = doc.add_paragraph()\n",
        "\n",
        "    # Escape regex special characters in keywords\n",
        "    escaped_keywords = [re.escape(keyword) for keyword in keywords]\n",
        "\n",
        "    # Create a regex pattern to find keywords, considering word boundaries\n",
        "    pattern = r'\\b(?:' + '|'.join(escaped_keywords) + r')\\b'\n",
        "\n",
        "    # Split the text by the pattern to preserve the original text for non-matches\n",
        "    parts = re.split(pattern, article, flags=re.IGNORECASE)\n",
        "\n",
        "    # Use regex to find all matches\n",
        "    matches = re.findall(pattern, article, flags=re.IGNORECASE)\n",
        "\n",
        "    match_iter = iter(matches)\n",
        "\n",
        "    # Alternate between non-matches and matches\n",
        "    for part in parts:\n",
        "        paragraph.add_run(part)\n",
        "        try:\n",
        "            match = next(match_iter)\n",
        "            run = paragraph.add_run(match)\n",
        "            run.font.highlight_color = color\n",
        "            run.font.underline = True\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "    # Save the changes to the existing document\n",
        "    doc.save(doc_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "fyc_BKO5eN-a"
      },
      "outputs": [],
      "source": [
        "def card_cutter(task, article, document):\n",
        "  ## get key sentences\n",
        "  key_sentences = get_key_sentences(task, article)\n",
        "\n",
        "  highlighted = highlight(task, key_sentences)\n",
        "  final_cut(article, highlighted, document)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
